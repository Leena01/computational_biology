\documentclass[11pt,twocolumn]{article}
\usepackage{abstract}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\lstset{
    escapeinside={(*}{*)}
}

\usepackage{hyperref}
\lstset{aboveskip=24pt,belowskip=24pt}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsmath}
\lstset{language=Python}
\usepackage[english]{babel}
\usepackage[numbers]{natbib}
\usepackage{titlesec}
\usepackage{subcaption}
\usepackage{float}
\usepackage[strings]{underscore}
\usepackage{etoolbox}
\usepackage{booktabs}

\newlength{\toprulewidth}
\setlength{\toprulewidth}{0.25ex}
\patchcmd{\toprule}% <cmd>
  {\heavyrulewidth}{\toprulewidth}% <search><replace>
  {}{}% <success><failure>
  
\apptocmd{\sloppy}{\hbadness 10000\relax}{}{}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

\newcommand{\dd}[1]{\mathrm{d}#1}
\setlength{\columnsep}{0.75cm}

\newcommand*\ttvar[1]{\texttt{\expandafter\dottvar\detokenize{#1}\relax}}
\newcommand*\dottvar[1]{\ifx\relax#1\else
  \expandafter\ifx\string.#1\string.\allowbreak\else#1\fi
  \expandafter\dottvar\fi}
\usepackage{url}

% abstract styling
\renewenvironment{abstract}%
{%
  \centerline%
  {\large\bf Abstract}%
  \begin{quote}%
}
{
  \par%
  \end{quote}%
  \vskip 1ex%
}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=black,
    citecolor=black,
    urlcolor=blue,
}


\title{\textbf{Comparison of phylogenetic reconstruction methods}}
\author{Lívia Qian\\KTH Royal Institute of Technology\\School of Electrical Engineering and Computer Science\\liviaq@kth.se}

\begin{document}

\twocolumn[
  \begin{@twocolumnfalse}
	\date{}
	\maketitle

	\begin{abstract}
The aim of this work is to briefly describe and compare the performance of five methods for phylogenetic reconstruction. Instead of trying to be a comprehensive guide summarizing the theoretical background of phylogenetics, it focuses on the main principles of creating simple phylogenetic trees and the conclusions I reached while testing these methods. My primary goal was to measure the time complexity of each algorithm besides estimating the accuracy of the generated trees. In the end, the results were compared those of the paper \textit{Confirming the Phylogeny of Mammals by Use of Large Comparative Sequence Data Sets} (Prasad et al., 2008).
    \end{abstract}
  \end{@twocolumnfalse}
]

\section{Introduction}

Phylogenetics is the study of the evolutionary history of species and groups of species. There are multiple ways to express the relationship between different groups, one of them being the construction of phylogenetic trees (or evolutionary trees) where all taxa are present as leaves. Common traits can be observed using DNA sequences or morphology, where the latter is generally more complex due to the many characteristics a certain species can have. Phylogenetic trees can be rooted and unrooted; the former is a version representing not only the similarity between different taxa but the ancestral relationships as well, while the latter only illustrates the connection between the taxonomical units.

\section{Background}

DNA is a molecule that has two polynucleotide chains forming a double helix and carries information about an organism's development and reproduction. Genomes — genetic materials that make up an organism's body — consist of both protein-coding DNA (genes) and non-coding DNA. There are four different types of nucleoids — adenine (A), guanine (G), cytosine (C) and thymine (T) — that can define the characteristics of a DNA. DNA sequencing is the process of determining the order of such nucleoids in order to gain relevant information about the molecule at hand.

DNA sequences need to be of the same length in order to be comparable with each other, therefore a proper sequence alignment is needed before one can start constructing phylogenetic trees. Sequence alignment is a way of arranging multiple DNA sequences into a matrix so that regions that show a high degree of similarity in multiple sequences are placed underneath each other. As this process can make the characters in the sequences shift, a special character (commonly a "-") is used to fill in the gaps.

There are two main categories of tree-generating algorithms \cite{DeBruyn2013}: distance-based methods are based on measuring the difference between pairs of sequences, while character-based methods use the columns of a sequence alignment directly. Examples of distance-based algorithms include neighbor joining (NJ) \cite{1987}, UPGMA \cite{sokal58}, WPGMA \cite{sokal58} and BIONJ \cite{Gascuel1997}, while the two most widely used character-based methods are the maximum parsimony (MP) and maximum likelihood (ML) methods. While all these techniques are popular, distance methods are usually recommended when computational time is an important factor; in other cases, parsimony and likelihood are preferred as they are more rigorous and have the ability to explore different combinations.

\section{Methods}

\subsection{Distance-based methods}

\subsubsection{Neighbor joining}

Neighbor joining (NJ) is a distance-based method created by Saitou and Sei in 1987 \cite{1987}. A distance matrix is needed for all of the calculations in this method; this matrix should contain the pairwise distances between the input sequences according to a predefined distance metric. This matrix is reduced in size after each step because of the clustering nature of the method.

In each iteration, the following steps are repeated until there are only two nodes left:

\begin{enumerate}
\item Calculate the divergence ($r$) for every element. Divergence for element $a$ is the sum of the distances from $a$ to all the other elements.

\item Calculate the new, alternative distance matrix using the following formula:
\begin{align}
d_{new}(i, j) = d(i, j) - \frac{r(i) + r(j)}{N - 2}
\end{align}
where $N$ is the number of elements in the current iteration.

\item Take the smallest value in the new distance matrix and the elements belonging to it (let them be denoted by $a$ and $b$). Let $p$ be a new node that represents the common parent of $a$ and $b$. The length of the new branches can be calculated as
\begin{align}
\delta(a, p) = \frac{d(a, b)}{2} + \frac{r(a) - r(b)}{2 (N - 2)} \\
\delta(b, p) = d(a, b) - \delta(a, p)
\end{align}

\item The rows and columns belonging to $p$ need to be added to the original distance matrix, while the ones belonging to $a$ and $b$ need to be deleted. After this, the difference between $p$ and the remaining nodes need to be calculated. Let's take an arbitrary element $c$ as an example:
\begin{align}
d(c, p) = \frac{d(a, c) + d(b, c) - d(a, b)}{2}
\end{align}
Branches $(a, p)$ and $(b, p)$ are now part of the tree that is being built.

\end{enumerate}

After this, the remaining two elements can be joined by taking the ancestral node created in the last iteration and making the other node its child with their distance — that was specified in the last iteration — as the branch length.

Although it was originally intended for unrooted trees, NJ can easily be extended to rooted trees. This is one of the simplest and fastest methods in phylogeny, but it may not produce accurate results due to its simplicity.

\subsubsection{WPGMA}

Originally attributed to Sokal and Michener \cite{sokal58}, WPGMA (weighted pair group method with arithmetic mean) is a hierarchical clustering method that uses a distance matrix in a similar fashion as the neighbor joining method. Each step consists of the following and is repeated until there is only one node left:

\begin{enumerate}
\item Find the smallest element in the distance matrix. Suppose that it is $d(a, b)$, therefore it belongs to elements $a$ and $b$ (both of them can be clusters). Create a parent node called $p$.

\item Calculate branch lengths $\delta(a, p)$ and $\delta(b, p)$. If both $a$ and $b$ are leaves, these lengths are simply
\begin{align}
\delta(a, p) = \delta(b, p) = \frac{d(a, b)}{2}
\end{align}
If $a$ or $b$ is a cluster, the height of the subtree that belongs to them needs to be subtracted from $\frac{d(a, b)}{2}$ to get the  corresponding branch length. Suppose that $a$ is a cluster and that $c$ is a leaf of the subtree belonging to it. In this case,
\begin{align}
\delta(c, p) = \frac{d(a, b)}{2}
\end{align}
\begin{align}
\begin{split}
\delta(a, p) = \delta(c, p) - \delta(c, a) = \\
= \frac{d(a, b)}{2} - \delta(c, a)
\end{split}
\end{align}

In the case where $a$ is a leaf, $c$ could be represented by $a$ itself and $\delta(c, a)$ would be zero, therefore this is a generalized formula that can be used in any situation.

\item After cluster $p = (a, b)$ is created, it should be integrated into the distance matrix as a new element; for this purpose, a new column and a new row should be created and the columns and rows belonging to $a$ and $b$ may be deleted. The distance between $p$ and any arbitrary element $q$ can be calculated as
\begin{align}
\begin{split}
d(p, q) = d((a, b), q) = \\
= \frac{d(a, q) + d(b, q)}{2}
\end{split}
\end{align}
This step is not needed in the last iteration.

\end{enumerate}

\subsubsection{UPGMA}

Also created by Sokal and Michener \cite{sokal58}, UPGMA (unweighted pair group method with arithmetic mean) is highly similar to WPGMA. The only difference lies in that it keeps track of the number of elements in each cluster, making the resulting calculations unweighted. In Step 3, UPGMA uses cluster weights, resulting in the following distance matrix update rule:
\begin{align}
d((a, b), q) = \frac{d(a, q) \cdot m + d(b, q) \cdot n}{m + n}
\end{align}
where $a$ consists of $m$ and $b$ consists of $n$ elements. This is called \textit{proportional averaging}.

There are situations in which it is not recommended to use UPGMA, e.g., when the three-point criterion is not fulfilled. This criterion demands that for any three elements $d(a, c) \le max(d(a, b), d(b, c))$, i.e., that the two greatest distances be equal. This is needed because UPGMA assumes a constant rate of evolution across lineages, therefore none of the leaves can be further away from the root than the others.

\subsection{Character-based methods}
\subsubsection{Maximum parsimony}

Maximum parsimony (MP) is an umbrella term for all the methods that are based on the principle that a model is better than another if it implies fewer evolutionary events. The number of evolutionary events is represented by a cost called the \textit{parsimonious score}. There are two major categories: \textit{exact} methods are the ones that consider the score of all possible trees, thus giving a global minimum, and \textit{heuristic} methods are those that use some kind of technique to find a significant subset of all the trees and produces a somewhat sub-optimal result. As exact methods are often computationally infeasible, it is recommended to use a heuristic method and let it run for a sufficient amount of time.

Fitch's algorithm \cite{Fitch1971} can be used to calculate the score of an input tree  \cite{Ortiz2016}\cite{Stoye2009}: the first-pass of the algorithm starts from the leaves and goes up to the root to determine the parsimonious score and possible sets of bases at each internal node, while the second-pass goes from top to bottom and helps in determining a hypothetical tree. In each step of the first phase, the algorithm takes a look at the bases belonging to the children nodes (which consist of one element in the case of the leaves), and then, assigns their intersection to the parent node if they have bases in common. If they do not have any bases in common, the parent node gets the union of the children's sets and the parsimonious score is increased by 1. This algorithm is repeated for each site — that is, position in the sequences — in all input sequences and the score is summed up in the end.

There is a number of heuristic methods that can help in exploring new trees, for example nearest neighbor interchange (NNI), subtree pruning and regrafting (SPR), or tree bisection and reconnection (TBR) \cite{felsenstein2004}. NNI is the simplest one but is used for unrooted binary trees; the current tree may produce two new trees if all four branches connecting to one of its internal branches are swapped in every possible combination. In SPR, a certain subtree is removed from the current tree and gets reinserted elsewhere. TBR does the same as SPR, but instead of reinserting the subtree in the form it was in originally, it tries every possible way of connecting the two subtrees; similarly to NNI, it works for unrooted trees only. These methods are stochastic, but they should be able to approximate the original distribution of trees given that they have enough time to run.

Branch and bound, an exact method that filters out a certain number of trees, may reduce the running time of the exhaustive search significantly. This is a method that considers all possible trees but makes use of the fact that adding a new edge to a tree increases its parsimonious score, thus avoids going for trees that are bigger than the current one in each step.

\subsubsection{Maximum likelihood}

Maximum likelihood (ML), like in many cases where probabilistic methods are used, makes use of the basics of Bayesian statistics and examines the probability of a certain sequence given a model; the higher the probability the better the model. In some cases, the relationship between successive sites — characters — is the driving factor in determining the best model, e.g., in Hidden Markov models, where transition matrices are used for storing such information. It may have a high algorithmic complexity as evaluating one sequence in itself can be computationally intensive, let alone multiple sequences. Additionally, there are Bayesian methods (e.g., Markov chain Monte Carlo) that build upon ML; the major difference between these two groups of tree-building algorithms is that Bayesian methods take prior knowledge into account.

\subsection{Self-growing tree algorithm}

Self-growing tree algorithm (or self-organizing tree algorithm) is based on a paper published in 1997 by Dopazo and Carazo \cite{Dopazo1997} and is a combination of the Kohonen self-organizing map \cite{58325} and the growing cell structures algorithm of Fritzke \cite{Fritzke1994}. This is an unsupervised learning network that first initializes a tree consisting of a small number of nodes (the paper mentions two sister nodes) and then alternates between growing the tree and adapting it to the input sequences until it is fully grown and every taxonomic unit is assigned a proper place. The strictest exit condition guarantees that every input sequence is associated to a unique cell.

First, the input sequences are converted to one-hot encoded values. Secondly, the tree's initial node(s) and the corresponding weight matrices that the encoded input sequences will ultimately be compared to need to be created; in order to distinguish inner nodes from leaves, the authors of the paper mention that it would be best to call them \textit{nodes} and \textit{cells}, respectively. In each step, nodes are considered "closed", meaning that they cannot be assigned sequences after they transition from being a cell to a node. After the first few cells are initialized with numbers ranging from 0 to 1, the alternating phases begin to take place. The first phase is called adaptation, which is basically what characterizes Kohonen's self-organizing maps: the input points are compared to all available cells and those that are closest to each of the inputs are updated, along with their neighborhood (in the paper, finding the winning cell and updating its neighborhood is referred to as a \textit{presentation}). After running this for a number of epochs, the inputs are mapped to appropriate positions in the output space, namely, the cells that they are the closest to according to a predefined metric. This distance metric is defined as

\begin{align}
d_{S_j C_j} = \frac{\sum_{r = 1}^A S_j(r, l) \cdot C_i(r, l)}{L}
\end{align}

where $S_k$ is sequence $k$, $C_i$ is cell $i$, $A$ is the number of characters in the alphabet (in our case, four) and $L$ is the length of the sequences.

The concept of neighborhood is not as intuitive as when the output space is a 1D or 2D space; in the case where the winning cell's sister is a cell, the winning cell, the sister cell and their common ancestor (mother cell) all get updated — it should be noted that all three of them are assigned different update rates. However, if the sister is actually a subbranch of the tree and consists of multiple nodes and cells, only the winning cell is updated.

The second phase is the growing of the tree. As it is mentioned in the paper \cite{Dopazo1997}, "the growing of the network takes place in the cell having higher resources. This cell gives rise to two new descendant cells [...] and transforms itself into a node. At this moment, its upper node becomes a grandmother node, and thereafter, it does not receive any more updating. The two new cells are, in principle, identical to the node which generated them".

The tree stops growing when the resource value of the cell with the highest value is smaller than a predefined threshold — this threshold should be zero if the goal is to map each sequence to a unique cell, or rather, a sufficiently small number in order to avoid inaccuracies in numerical calculations. The concept of resource value is related to the distance between cells and input points, and is defined as

\begin{align}
R_i = \frac{\sum_{k = 1}^K d_{S_k C_i}}{K}
\end{align}

where $R$ is the resource value belonging to cell $i$, $S_k$ is sequence $k$, $C_i$ is cell $i$ and the summation is done over the $K$ sequences associated to cell $i$.

The criteria used for monitoring the convergence of the network relies on the definition of the total error, $\epsilon$, "defined as the summation of the distances of each sequence to the corresponding winning cell after an epoch" \cite{Dopazo1997}. This helps in setting an exit condition for the adaptation process — e.g., the process can be ended when the relative increase of the error falls below a small threshold. The relative increase in error is

\begin{align}
\Big|\frac{\epsilon_t - \epsilon_{t-1}}{\epsilon_{t-1}}\Big| < E
\end{align}

What is also interesting is that there is an extended update rule that is based on Kohonen's original formula:

\begin{align}
C_i(\tau + 1) = C_i(\tau) + \eta_{t, \tau, i} \cdot [S_j - C_i(\tau)]
\end{align}

and $\eta_{t, \tau, i}$ is defined as a factor dependent on the number of cycles among others:

\begin{align}
\eta_{t, \tau, i} = \alpha_i \cdot \frac{1 - t}{M_t} \cdot (1 - b\tau)
\end{align}

where $\alpha_i$ is a coefficient dependent on the role of the current cell, $t$ is the total number of presentations, $M_t$ is the maximum number of presentations allowed ($\mu \times A \times L$) and $b$ is the
slope for the reduction of the interaction as the number of presentations, $\tau$, increases within a cycle.

\section{Implementation}

I implemented the algorithms mentioned in the previous section using Python in combination with NumPy and Pandas. For reference and sanity tests, I used Biopython's implementation of some of the algorithms mentioned above. Since some of the data structures needed were not feasible to implement within the scope of the project, I decided to use Biopython's corresponding classes to facilitate the work; these are \ttvar{Bio.SeqIO}, \ttvar{Bio.Align.MultipleSeqAlignment}, \ttvar{Bio.Alphabet} and \ttvar{Bio.Phylo.BaseTree}. They were needed to read in data, create sequence alignments in the format needed, fill in the gaps in the sequence alignments and create the trees in a form that can be visualized easily, respectively.

I decided to use rooted trees because they are generally more accurate and more easily comparable in a lot of cases. Biopython's \ttvar{BaseTree} can easily be visualized with \ttvar{Bio.Phylo.draw}, a function that takes into account features like branch length and custom labels.

The methods I implemented are neighbor joining, UPGMA, WPGMA, maximum parsimony and SOTA. In the case of MP, I implemented an exact algorithm — with an additional option for branch and bound — that uses exhaustive search to find all the trees and a heuristic algorithm that uses SPR (I will refer to them as exact and heuristic methods, respectively). SOTA contains a few hyper-parameters; these can be tuned before extensive testing. I found that the following parameters provided a relatively fast convergence: $\alpha_w = 0.5$ (current cell's update rate), $\alpha_m = 0.02$ (mother cell's update rate), $\alpha_s = 0.01$ (sister cell's update rate), $b = 0$ (slope), $\eta = 0.5$ (learning rate).

\subsection{Maximum parsimony — versions}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{img/mp_9_taxa_heuristic.png}
    \caption{The result of MP run on 8 elements of the coding dataset. The result is the best tree after 50 iterations.}
    \label{fig:mp_8_taxa_heuristic}
\end{figure*}

While the exact method doesn't need any find-tuning (branch and bound produces the same result as the full version), heuristic MP with SPR needs the maximum number of iterations — that is, the number of trees to test — as input. The heuristic version needs to be run for a relatively long time to find a sufficiently good result; as it is stochastic, the use of optimization methods may boost its performance or speed. For the remaining experiments, I decided to test the exact method instead of the heuristic one as it is able to produce exact results.

Figure \ref{fig:mp_8_taxa_heuristic} shows a tree resulting from a 50-iteration heuristic MP run on 9 randomly selected species; although the algorithm managed to find some smaller clusters (\textit{baboon} and \textit{squirrel monkey}, \textit{mouse} and \textit{bat}), it is still far from optimal.

%\subsection{Hyper-parameters of SOTA}
% TODO
%In the case of the self-organizing tree algorithm, three $\alpha$ values (for the current cell, sister cell and mother cell), 

\section{Data}

The first dataset I used is a protein-coding sequence alignment \cite{dataset} consisting of 44 sequences of different vertebrate taxa. The input is first processed by Biopython's sequence alignment class before being passed to any of the algorithms implemented. Since this dataset was evaluated in a 2008 paper by Prasad et al. \cite{Prasad2008}, I decided to compare my results with those published in this work. From now on, I will refer to this dataset as the \textit{coding dataset}. The other dataset I ran my experiments on uses non-coding DNA sequences but is identical to the coding dataset in every other aspect \cite{dataset}. I will refer to this as the \textit{non-coding dataset}. For the experiments I conducted, I assumed that the relationship between the speed of my algorithms holds for different datasets.

\section{Experiments}

%TODO
First, I tested what parameters or version of a certain algorithm would be best for the final tests.

\section{Results}

\subsection{Comparison of running time}

All the experiments were run on CPU (Intel Core i7-8650U, 1.9 GHz); none of the algorithms were optimized for GPU which made testing hard to a certain extent. Some methods were computationally intensive (e.g, maximum parsimony with 9 species and SOTA with 44 species), therefore I will include the result of only one test run in these cases. The results in this section were produced using the coding dataset.

\begin{table}[H]
\label{table:distance}
  \caption{Running time of the distance-based methods on the coding dataset. 44 taxa are used. Numbers are based on three test runs.}
  \centering
  \begin{tabular}{lll}
    \toprule
    Method & Mean (seconds) & Std. \\
    \midrule
    NJ & 3.3105 & 0.3885 \\
    UPGMA & 0.6767 & 0.0446 \\
    WPGMA & 0.5359 & 0.0526 \\	
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
\label{table:mp}
  \caption{Running time of the exact maximum parsimony method on the coding dataset. Uses branch and bound. Based on three test runs (except for the case with 9 taxa).}
  \centering
  \begin{tabular}{lll}
    \toprule
    Nr. of taxa & Mean (seconds) & Std. \\
    \midrule
    6 & 43.9988 & 2.7520 \\
    7 & 916.6252 & 661.2072 \\
    8 & 8613.8869 & 5145.0788 \\
    9 & 228420.7188 & - \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
\label{table:sota}
  \caption{Running time of the self-growing tree algorithm on the coding dataset. Based on three test runs (except for the case with 44 taxa).}
  \centering
  \begin{tabular}{lll}
    \toprule
    Nr. of taxa & Mean (seconds) & Std. \\
    \midrule
    4 & 53.8339 & 4.49524 \\
    5 & 93.3756 & 3.4764 \\
    15 & 2399.81665 & 464.18335 \\
    44 & 36669.3622 & - \\
    \bottomrule
  \end{tabular}
\end{table}

%TODO run for 15

It can be seen that the three distance-based methods are immensely fast, especially since they do not entail convoluted tree-manipulation steps. SOTA runs in a reasonable amount of time, and it may be reasonable to think that it can provide a better solution than NJ, UPGMA and WPGMA while still being able to keep its running time under control.

\subsection{Comparison of accuracy}

\subsubsection{Coding dataset}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/nj.png}
    \caption{The result of neighbor joining run on the coding dataset.}
    \label{fig:nj}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/upgma.png}
    \caption{The result of UPGMA run on the coding dataset.}
    \label{fig:upgma}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/wpgma.png}
    \caption{The result of WPGMA run on the coding dataset.}
    \label{fig:wpgma}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/mp_9_taxa.png}
    \caption{The result of exact MP with branch and bound run on 9 elements of the coding dataset. Branch length have been adjusted to the parsimonious scores of the nodes.}
    \label{fig:mp}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/sota.png}
    \caption{The result of SOTA run on the coding dataset.}
    \label{fig:sota}
\end{figure*}

I first tested all five algorithms on the coding dataset. Regarding the result of neighbor joining (Figure \ref{fig:nj}), we can see that \textit{cats} and \textit{ferrets} are closely related (Carnivora, Laurasiatheria, Boreoeutheria), but they are further away from \textit{dogs} and \textit{pigs} which should be closely related to them according to the point of reference \cite{Prasad2008}. \textit{Sheep} and \textit{cows} are related (Cetartiodactyla), as well as species that are close to \textit{humans} (baboon, orangutan, chimp, gorilla, colobus monkey, macaque, gibbon). Neighbor joining managed to position other primates in the same cluster as well. The algorithm did well by placing some rodents in the vicinity of primates; it, however, put \textit{rats} and \textit{mice} into a different high-level category as humans. Other discrepancies include putting mostly dissimilar species like \textit{muntjak} and \textit{clouded leopard} next to each other but it can be said that it performed fairly well, especially since it is the most basic method.

When it comes to UPGMA (Figure \ref{fig:upgma}), cats and ferrets are still far from dogs and pigs but munjaks and leopards moved further from each other. Besides this, it performed a little worse than neighbor joining; \textit{chimps} and \textit{orangutans} have moved away from other primates, and other high-level categories remained mixed or obscure. Closely tied species still go together — \textit{tetraodon-fugu}, \textit{sheep-cow}, \textit{baboon-macaque}, \textit{rat-mouse}, \textit{human-gorilla} —, but other highly similar species — \textit{muntjak-cow}, \textit{horseshoe bat and short-tailed bat}, \textit{mouse lemur and right-tailed lemur}, \textit{owl money and squirrel monkey}, \textit{short-tailed opossum (monodelphis) and North American opossum (opossum)} — are separated from each other.

In the case of WPGMA (Figure \ref{fig:wpgma}), only the mid-level clusters seem to have moved around compared to the structure of the tree produced by UPGMA. Most of the statements that apply to UPGMA are true of WPGMA as well. Slight improvements can be seen in certain relationships, e.g. \textit{platypuses} and \textit{chickens} got paired up and \textit{guinea pigs} got closer to \textit{rats} and \textit{mice}; these changes, however, are not sufficient to claim that there is a significant difference in UPGMA and WPGMA.

As mentioned before, time is an important factor when using maximum parsimony. The maximum number of taxa I ran the exact method on was 9, therefore only a subtree of the result is shown in Figure \ref{fig:mp}. The exact method has the capacity to evaluate every possible combination of branches, therefore the solution it provides is optimal. It can be said that the resulting tree bears a high degree of resemblance to Figure 1 in Prasad's paper \cite{Prasad2008}.

Finally, SOTA (Figure \ref{fig:sota}) created a tree that has guessed the major clusters right, but there are minor disparities in less obvious cases: \textit{muntjak} is grouped with \textit{chicken} and \textit{platypus}, \textit{squirrel monkey} is with \textit{pig} and \textit{dog} and \textit{tenrec} belongs to the same branch as \textit{clouded leopard}. It was generally good at identifying the similarities between different primate and rodent species, but there is still room for improvement, for example, \textit{shrew} and one of the \textit{bat} species have diverged from the other species of \textit{Laurasiatheria}.

\subsubsection{Non-coding dataset}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/nj_noncoding.png}
    \caption{The result of neighbor joining run on the non-coding dataset.}
    \label{fig:nj_noncoding}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/upgma_noncoding.png}
    \caption{The result of UPGMA run on the non-coding dataset.}
    \label{fig:upgma_noncoding}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/wpgma_noncoding.png}
    \caption{The result of WPGMA run on the non-coding dataset.}
    \label{fig:wpgma_noncoding}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/mp_8_taxa_noncoding_1.png}
    \caption{The result of exact MP with branch and bound run on 8 elements of the non-coding dataset. Branch length have been adjusted to the parsimonious scores of the nodes.}
    \label{fig:mp_noncoding_1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/mp_8_taxa_noncoding_2.png}
    \caption{The result of exact MP with branch and bound run on 8 elements of the non-coding dataset. Branch length have been adjusted to the parsimonious scores of the nodes.}
    \label{fig:mp_noncoding_2}
\end{figure*}

Neighbor joining performed quite well (Figure \ref{fig:nj_noncoding}); primates are in the same cluster except for \textit{squirrel monkey} and \textit{Glires} (hares, rabbits and rodents) are also together except for \textit{rabbit}, which was instead linked with primates. Cats are still far from dogs, and rats and mice are far from humans, too.

UPGMA (Figure \ref{fig:upgma_noncoding}) produced slightly better results that neighbor joining; the main categories seem stable, although some elements in Laurasiatheria and Glires have been moved around. The tree created with WPGMA (Figure \ref{fig:wpgma_noncoding}) is also remarkable, only some species like \textit{horse} and \textit{vervet}, \textit{dusky titi} and \textit{elephant} deviated from their expected places.

For the sake of simplicity, I decided to make two test runs on 8 taxa instead of one test run on 9 taxa in the case of maximum parsimony. Figure \ref{fig:mp_noncoding_1} is very similar to Figure \ref{fig:mp}; species were chosen to coincide with the ones shown in the non-coding run and as it can be seen there is little or no difference between the two types of sequences in the case of the species selected. Figure \ref{fig:mp_noncoding_2}, although there is no point of comparison from the coding dataset, shows a tree that seems to contain mostly bad taxonomic relationships: according to the tree, primates have diverged early on while \textit{squirrel monkey} is closer to the \textit{guinea pig} than any other primate. This may be due to the lack of information in the non-coding dataset, which contradicts what has been mentioned in correlation with Figure \ref{fig:mp_noncoding_1}; in order to determine if it is really the case, further experimentation with different combinations of species is needed.

%TODO point of comparison from the coding dataset?

\section{Conclusion}

Among distance-based methods, WPGMA performed slightly better than NJ and UPGMA on the two datasets. Exact MP with branch and bound could be a good choice as it has produced mostly accurate results, but its running time creates an obstacle in cases with a high number of taxa; heuristic MP with SPR could be a good substitute but there is no guarantee that the sub-optimal result it finds is a sufficiently good one. SOTA is not significantly better than the distance-based methods and its computational complexity is a hindrance to accuracy.

Combining these methods by e.g. starting out from a tree pre-calculated with neighbor joining and then finding better solutions with heuristic MP could result in a possible improvement in performance; such optimization techniques are often used in the field of phylogenetics \cite{Zvelebil2007}. Additionally, it is possible that ML methods give better results than MP methods under special circumstances; finding out which one is better than the other for specific datasets needs extensive experimentation, which I leave for future work.

The repository is available on \href{https://github.com/Leena01/computational_biology}{GitHub}. Since the project provides a simplified overview of various phylogenetic methods, I hope that it can serve as reference for others who are interested in this field.

\newpage
\hypersetup{
	urlcolor=black
}
\bibliographystyle{unsrtnat}
\bibliography{report}

\newpage
\appendix
\section{Functionality tests}

\end{document}
